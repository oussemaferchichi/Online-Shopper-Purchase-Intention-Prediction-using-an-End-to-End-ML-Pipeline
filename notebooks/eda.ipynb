{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Online Shoppers Purchasing Intention - Exploratory Data Analysis\n",
    "\n",
    "**Course**: Python for Data Science ‚Äì Guided Machine Learning  \n",
    "**Week**: 1 - Exploratory Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook performs exploratory data analysis on the Online Shoppers Purchasing Intention dataset to:\n",
    "1. Understand the dataset structure and quality\n",
    "2. Identify missing values and data issues\n",
    "3. Analyze the class distribution (Revenue)\n",
    "4. Explore relationships between features and purchase behavior\n",
    "5. Document a preprocessing plan for Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/online_shoppers_intention.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]:,}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "print(\"Column Names:\")\n",
    "print(\"=\" * 50)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types and non-null counts\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types summary\n",
    "print(\"\\nData Types Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "if len(missing_df) == 0:\n",
    "    print(\"‚úÖ No missing values found in the dataset!\")\n",
    "else:\n",
    "    print(missing_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical features\n",
    "print(\"Statistical Summary of Numerical Features:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 6. Class Distribution Analysis (Revenue)\n",
    "\n",
    "**Key Question**: How balanced is our target variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Revenue distribution\n",
    "revenue_counts = df['Revenue'].value_counts()\n",
    "revenue_percentages = df['Revenue'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Revenue Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"No Purchase (FALSE): {revenue_counts[False]:,} ({revenue_percentages[False]:.2f}%)\")\n",
    "print(f\"Purchase (TRUE):     {revenue_counts[True]:,} ({revenue_percentages[True]:.2f}%)\")\n",
    "print(f\"\\n‚ö†Ô∏è  CLASS IMBALANCE DETECTED!\")\n",
    "print(f\"Purchase rate is only {revenue_percentages[True]:.2f}%\")\n",
    "print(f\"This justifies the use of SMOTE for handling class imbalance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "revenue_counts.plot(kind='bar', ax=axes[0], color=colors, edgecolor='black', alpha=0.8)\n",
    "axes[0].set_title('Revenue Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Revenue', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(['No Purchase', 'Purchase'], rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(revenue_counts.values):\n",
    "    axes[0].text(i, v + 200, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(revenue_counts.values, labels=['No Purchase', 'Purchase'], autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90, explode=(0.05, 0.05), shadow=True)\n",
    "axes[1].set_title('Revenue Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/plots/01_class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Plot saved: data/plots/01_class_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 7. Purchase Rate by Visitor Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze purchase rate by visitor type\n",
    "visitor_revenue = pd.crosstab(df['VisitorType'], df['Revenue'], normalize='index') * 100\n",
    "\n",
    "print(\"Purchase Rate by Visitor Type:\")\n",
    "print(\"=\" * 50)\n",
    "print(visitor_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize purchase rate by visitor type\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "visitor_revenue.plot(kind='bar', ax=ax, color=['#e74c3c', '#2ecc71'], \n",
    "                      edgecolor='black', alpha=0.8)\n",
    "ax.set_title('Purchase Rate by Visitor Type', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Visitor Type', fontsize=12)\n",
    "ax.set_ylabel('Percentage (%)', fontsize=12)\n",
    "ax.set_xticklabels(visitor_revenue.index, rotation=45, ha='right')\n",
    "ax.legend(['No Purchase', 'Purchase'], title='Revenue')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.1f%%', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/plots/02_purchase_by_visitor_type.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Plot saved: data/plots/02_purchase_by_visitor_type.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 8. Purchase Rate by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze purchase rate by month\n",
    "month_revenue = pd.crosstab(df['Month'], df['Revenue'], normalize='index') * 100\n",
    "\n",
    "# Sort by month order\n",
    "month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'June', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "month_revenue = month_revenue.reindex([m for m in month_order if m in month_revenue.index])\n",
    "\n",
    "print(\"Purchase Rate by Month:\")\n",
    "print(\"=\" * 50)\n",
    "print(month_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize purchase rate by month\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "month_revenue[True].plot(kind='bar', ax=ax, color='#3498db', \n",
    "                          edgecolor='black', alpha=0.8)\n",
    "ax.set_title('Purchase Rate by Month', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Month', fontsize=12)\n",
    "ax.set_ylabel('Purchase Rate (%)', fontsize=12)\n",
    "ax.set_xticklabels(month_revenue.index, rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(month_revenue[True].values):\n",
    "    ax.text(i, v + 0.5, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Add horizontal line for average purchase rate\n",
    "avg_purchase_rate = month_revenue[True].mean()\n",
    "ax.axhline(y=avg_purchase_rate, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Average: {avg_purchase_rate:.1f}%')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/plots/03_purchase_by_month.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Plot saved: data/plots/03_purchase_by_month.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 9. Distribution of Key Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distributions of key numerical features\n",
    "key_features = ['PageValues', 'BounceRates', 'ExitRates']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    # Histogram with KDE\n",
    "    axes[i].hist(df[feature], bins=50, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel(feature, fontsize=10)\n",
    "    axes[i].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = df[feature].mean()\n",
    "    median_val = df[feature].median()\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[i].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/plots/04_numerical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Plot saved: data/plots/04_numerical_distributions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots by Revenue\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    df.boxplot(column=feature, by='Revenue', ax=axes[i], \n",
    "               patch_artist=True, \n",
    "               boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "               medianprops=dict(color='red', linewidth=2))\n",
    "    axes[i].set_title(f'{feature} by Revenue', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Revenue', fontsize=10)\n",
    "    axes[i].set_ylabel(feature, fontsize=10)\n",
    "    axes[i].set_xticklabels(['No Purchase', 'Purchase'])\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('')  # Remove the default title\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/plots/05_boxplots_by_revenue.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Plot saved: data/plots/05_boxplots_by_revenue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 10. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns only\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Convert Revenue to numeric for correlation\n",
    "df_corr = df.copy()\n",
    "df_corr['Revenue'] = df_corr['Revenue'].astype(int)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_corr[numerical_cols + ['Revenue']].corr()\n",
    "\n",
    "# Get correlations with Revenue\n",
    "revenue_correlations = correlation_matrix['Revenue'].sort_values(ascending=False)\n",
    "print(\"Correlation with Revenue:\")\n",
    "print(\"=\" * 50)\n",
    "print(revenue_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap - Focus on Revenue', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/plots/06_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Plot saved: data/plots/06_correlation_heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlations with Revenue (bar plot)\n",
    "top_n = 10\n",
    "top_correlations = revenue_correlations[1:top_n+1]  # Exclude Revenue itself\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in top_correlations.values]\n",
    "top_correlations.plot(kind='barh', color=colors, edgecolor='black', alpha=0.8)\n",
    "plt.title(f'Top {top_n} Features Correlated with Revenue', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(top_correlations.values):\n",
    "    plt.text(v + 0.01 if v > 0 else v - 0.01, i, f'{v:.3f}', \n",
    "             va='center', ha='left' if v > 0 else 'right', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/plots/07_top_correlations.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Plot saved: data/plots/07_top_correlations.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 11. Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"KEY INSIGHTS FROM EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW:\")\n",
    "print(f\"   - Total Records: {df.shape[0]:,}\")\n",
    "print(f\"   - Total Features: {df.shape[1]}\")\n",
    "print(f\"   - Missing Values: {df.isnull().sum().sum()} (0%)\")\n",
    "\n",
    "print(\"\\n2. CLASS IMBALANCE (CRITICAL):\")\n",
    "purchase_rate = (df['Revenue'].sum() / len(df)) * 100\n",
    "print(f\"   - Purchase Rate: {purchase_rate:.2f}%\")\n",
    "print(f\"   - No Purchase Rate: {100 - purchase_rate:.2f}%\")\n",
    "print(f\"   - Imbalance Ratio: ~1:{int(100/purchase_rate)}\")\n",
    "print(f\"   ‚ö†Ô∏è  SMOTE will be ESSENTIAL for handling this imbalance!\")\n",
    "\n",
    "print(\"\\n3. VISITOR TYPE INSIGHTS:\")\n",
    "for visitor_type in df['VisitorType'].unique():\n",
    "    vt_purchase_rate = (df[df['VisitorType'] == visitor_type]['Revenue'].sum() / \n",
    "                        len(df[df['VisitorType'] == visitor_type])) * 100\n",
    "    print(f\"   - {visitor_type}: {vt_purchase_rate:.2f}% purchase rate\")\n",
    "\n",
    "print(\"\\n4. TOP CORRELATED FEATURES WITH REVENUE:\")\n",
    "for i, (feature, corr) in enumerate(revenue_correlations[1:6].items(), 1):\n",
    "    print(f\"   {i}. {feature}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\n5. SEASONAL PATTERNS:\")\n",
    "best_month = month_revenue[True].idxmax()\n",
    "worst_month = month_revenue[True].idxmin()\n",
    "print(f\"   - Highest purchase rate: {best_month} ({month_revenue[True].max():.2f}%)\")\n",
    "print(f\"   - Lowest purchase rate: {worst_month} ({month_revenue[True].min():.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EDA COMPLETE - Ready for Preprocessing Planning!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìù Preprocessing Plan for Week 2\n",
    "\n",
    "## Overview\n",
    "\n",
    "Based on the EDA findings, the following preprocessing pipeline will be implemented using **scikit-learn Pipeline** to ensure reproducibility and prevent data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Handle Missing Values\n",
    "\n",
    "**Finding**: No missing values detected in the current dataset.\n",
    "\n",
    "**Strategy** (for robustness in case of missing values in future data):\n",
    "- **Numerical features**: Impute with **mean** or **median** (median preferred for skewed distributions)\n",
    "- **Categorical features**: Impute with **mode** (most frequent value)\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Numerical imputer\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Categorical imputer\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Encode Categorical Variables\n",
    "\n",
    "**Categorical Features Identified**:\n",
    "- `Month` (Jan, Feb, Mar, etc.)\n",
    "- `VisitorType` (New_Visitor, Returning_Visitor, Other)\n",
    "- `Weekend` (Boolean - already binary)\n",
    "\n",
    "**Strategy**:\n",
    "- Use **One-Hot Encoding** for `Month` and `VisitorType`\n",
    "- Convert `Weekend` and `Revenue` to binary (0/1)\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_features = ['Month', 'VisitorType']\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "```\n",
    "\n",
    "**Note**: `drop='first'` prevents multicollinearity by dropping one category.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Scale Numerical Features\n",
    "\n",
    "**Numerical Features** (different scales observed):\n",
    "- Page-related: `Administrative`, `Informational`, `ProductRelated`, etc.\n",
    "- Duration-related: `Administrative_Duration`, `Informational_Duration`, etc.\n",
    "- Behavior metrics: `BounceRates`, `ExitRates`, `PageValues`\n",
    "\n",
    "**Strategy**:\n",
    "- Use **StandardScaler** (z-score normalization)\n",
    "- Transforms features to have mean=0 and std=1\n",
    "- Required for distance-based algorithms (Logistic Regression, SVM, KNN)\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Train-Test Split\n",
    "\n",
    "**Strategy**:\n",
    "- **Split ratio**: 80% training, 20% testing\n",
    "- **Stratification**: Ensure equal class distribution in both sets\n",
    "- **Random state**: Set for reproducibility\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Handle Class Imbalance with SMOTE\n",
    "\n",
    "**Critical Finding**: Purchase rate is only **~15%** ‚Üí Severe class imbalance!\n",
    "\n",
    "**Strategy**:\n",
    "- Apply **SMOTE (Synthetic Minority Over-sampling Technique)**\n",
    "- Generates synthetic samples for minority class (purchases)\n",
    "- **IMPORTANT**: Apply SMOTE **only on training data** to prevent data leakage\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Why SMOTE?**\n",
    "- Prevents model from being biased toward majority class\n",
    "- Improves recall for minority class (purchase prediction)\n",
    "- Better than simple oversampling (no duplicates)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Build Scikit-Learn Pipeline\n",
    "\n",
    "**Integration Strategy**:\n",
    "- Combine all preprocessing steps into a single pipeline\n",
    "- Ensures consistent transformations during training and testing\n",
    "- Prevents data leakage\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_features = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]\n",
    "categorical_features = ['Month', 'VisitorType']\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        \n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(drop='first', sparse_output=False))\n",
    "        ]), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline (preprocessing + model)\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())  # Placeholder for Week 2\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Preprocessing Workflow Summary\n",
    "\n",
    "```\n",
    "Raw Data\n",
    "   ‚Üì\n",
    "1. Handle Missing Values (if any)\n",
    "   ‚Üì\n",
    "2. Encode Categorical Variables (One-Hot)\n",
    "   ‚Üì\n",
    "3. Scale Numerical Features (StandardScaler)\n",
    "   ‚Üì\n",
    "4. Train-Test Split (80/20, stratified)\n",
    "   ‚Üì\n",
    "5. Apply SMOTE (on training data only)\n",
    "   ‚Üì\n",
    "Ready for Model Training (Week 2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Considerations for Week 2\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Create interaction features (e.g., `BounceRate * ExitRate`)\n",
    "   - Binning of continuous variables if needed\n",
    "\n",
    "2. **Alternative Scaling Methods** (if needed):\n",
    "   - MinMaxScaler for neural networks\n",
    "   - RobustScaler for outlier-heavy features\n",
    "\n",
    "3. **Cross-Validation**:\n",
    "   - Use StratifiedKFold (5-10 folds) for robust evaluation\n",
    "\n",
    "4. **Evaluation Metrics** (for imbalanced data):\n",
    "   - **Precision, Recall, F1-Score** (more important than accuracy)\n",
    "   - **ROC-AUC** and **Precision-Recall AUC**\n",
    "   - **Confusion Matrix**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Week 1 Deliverables Complete\n",
    "\n",
    "1. ‚úÖ Dataset loaded and explored\n",
    "2. ‚úÖ Missing value analysis completed (none found)\n",
    "3. ‚úÖ Class imbalance identified and quantified (~15% purchase rate)\n",
    "4. ‚úÖ EDA visualizations created and saved\n",
    "5. ‚úÖ Preprocessing plan documented\n",
    "6. ‚úÖ Ready for Week 2 implementation\n",
    "\n",
    "**Next Steps**: Implement the preprocessing pipeline and begin model training in Week 2!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
